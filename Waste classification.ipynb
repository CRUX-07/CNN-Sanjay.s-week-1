

python
# Waste Classification using CNN in Jupyter Notebook

# Install required libraries
!pip install tensorflow matplotlib


### **1. Import Required Libraries**

```python
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
import matplotlib.pyplot as plt
```

### **2. Prepare the Dataset**

Ensure your dataset is available in the proper folder structure. The folder should look something like this:

```
/content/data/
    ├── organic/
    └── recyclable/
```

You can load the dataset like this:

```python
# Path to dataset (update path accordingly)
data_dir = '/content/data/'  # Modify this path based on your dataset location

# Data augmentation and preprocessing
train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2  # Split the dataset into training and validation sets
)

train_generator = train_datagen.flow_from_directory(
    data_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='binary',
    subset='training'  # Training data
)

validation_generator = train_datagen.flow_from_directory(
    data_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='binary',
    subset='validation'  # Validation data
)
```

### **3. Build the CNN Model**

Here’s a simple CNN architecture for classifying waste as organic or recyclable.

```python
# Build the CNN model
model = Sequential()

# First convolutional layer
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))
model.add(MaxPooling2D(pool_size=(2, 2)))

# Second convolutional layer
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

# Third convolutional layer
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

# Flatten the feature maps into a 1D vector
model.add(Flatten())

# Fully connected layers (Dense layers)
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))  # Dropout for regularization
model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
```

### **4. Train the Model**

Now that the model is defined, train it using the dataset.

```python
# Train the model
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // validation_generator.batch_size,
    epochs=10  # Adjust the number of epochs based on the dataset size
)
```

### **5. Evaluate the Model**

Check the performance of the model on the validation dataset.

```python
# Evaluate the model on validation data
val_loss, val_acc = model.evaluate(validation_generator)
print(f'Validation Accuracy: {val_acc:.2f}')
```

### **6. Save the Trained Model**

After training, save the trained model so that you can load it later without retraining.

```python
# Save the trained model
model.save('waste_classification_model.h5')
```

### **7. Plot Training and Validation Accuracy/Loss**

Visualize the training and validation accuracy and loss using `matplotlib`.

```python
# Plot training and validation accuracy/loss
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(10)  # Modify based on the number of epochs

plt.figure(figsize=(12, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()
```

### **8. Load the Model and Predict on New Images**

Once the model is trained, you can use it to predict new images.

```python
from tensorflow.keras.preprocessing import image
import numpy as np

# Load the saved model
model = tf.keras.models.load_model('waste_classification_model.h5')

# Function to predict waste type from an image
def predict_waste(image_path):
    img = image.load_img(image_path, target_size=(150, 150))
    img_array = image.img_to_array(img) / 255.0  # Normalize the image
    img_array = np.expand_dims(img_array, axis=0)  # Reshape for prediction

    prediction = model.predict(img_array)

    if prediction > 0.5:
        print(f'The image is Recyclable waste.')
    else:
        print(f'The image is Organic waste.')

# Example usage:
predict_waste('/content/data/organic/sample_image.jpg')  # Update path to your test image
```

---

### Summary of Code Blocks:

1. **Import Libraries**: You import `tensorflow`, `matplotlib`, and other necessary packages.
2. **Data Preprocessing**: You load the dataset using `ImageDataGenerator` and split it into training and validation sets.
3. **CNN Model**: You define a simple CNN model with multiple convolutional layers, max pooling, and dense layers.
4. **Model Training**: The model is trained on the data and you can visualize the accuracy and loss using graphs.
5. **Saving and Loading Model**: You save the model to reuse it later and predict waste types on new images.

---

You can now save this as a Jupyter notebook (`.ipynb`), run it, and upload it to GitHub. Let me know if you need any further help!
